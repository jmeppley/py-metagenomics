##########################
#
# format_gtdb.snake
#
# Formats the latest GTDB database for last and/or diamond
#  Uses prokka to predict and annotate genes
#
# first run download_gtdb.snake to get the necessary files
#
# a large number of files are written to the working directory in
# "GTDB/{release}"
##########################
import re
import pandas
from Bio import SeqIO
from edl.gtdb import generate_taxdump

# where to put the downloaded files:
seqdb_root = config.get('seqdb_root','seqdbs')
download_root = config.get('download_root', seqdb_root)

# version number
release = config.get('release', '89.0')
rel = release.split(".")[0]
fmt_threads = config.setdefault('fmt_threads', 20)

# already downloaded files
download_dir = os.path.join(download_root, 'GTDB', release)
seqdb_dir = os.path.join(seqdb_root, 'GTDB', release)
gtdb_tarball = "{download_dir}/gtdbtk_r{rel}_data.tar.gz".format(**locals())

# get list of genomes
tax_table = download_dir + "/sp_clusters_r89.tsv"
genome_tax_table = pandas.read_csv(tax_table, \
                                   sep='\t', \
                                   skiprows=config.get('skiprows', None),
                                   nrows=config.get('nrows', None),
                                   index_col=0, \
                                   usecols=[0,1,2], \
                                   names=['genome','species','lineage'], \
                                   header=0)
genome_taxonomy = {re.sub(r'^(RS|GB)_', '', g):t + ";" + s \
                   for g,s,t in genome_tax_table.itertuples()}
orgid_to_genome = {re.sub(r'\.\d+$', '', g):g \
                   for g in genome_taxonomy}
"""
genome_set = set(pandas.read_csv(tax_table, \
                                 sep='\t', \
                                 usecols=[0,], \
                                 index_col=0).index)
orgids = [re.sub(r'\.\d+$', '', g) for g in genome_set]
"""

# file names
outputs = config.setdefault('outputs', set())
work_dir = "GTDB/" + release
name_root = seqdb_dir + "/gtdb_r" + release
faa_file = name_root + ".faa"
header_file = name_root + ".headers"

# taxonomy files
tax_map_file = seqdb_dir + "/taxdump/gtdb.acc.to.taxid"
names_dmp = seqdb_dir + "/taxdump/names.dmp"
nodes_dmp = seqdb_dir + "/taxdump/nodes.dmp"

# formatting for last and diamond happends here
config['dbs'] = {'GTDB': {'fasta': faa_file, \
                          'is_prot': True,
                          'links': {'.tax': tax_map_file, \
                                    '.ids': True, \
                                    'names.dmp': names_dmp, \
                                    'nodes.dmp': nodes_dmp}}}
include: 'format_dbs.snake'

rule outputs:
    input: outputs

rule taxonomy:
    input:
        genes=faa_file
    output:
        names=names_dmp,
        nodes=nodes_dmp,
        tax_map=tax_map_file
    benchmark:
        'benchmarks/{}/taxonomy.time'.format(release)
    run:
        logger.debug("generate_taxdump('{input.genes}')")
        generate_taxdump(fasta=input.genes, \
                         dump=os.path.join(seqdb_dir, 'taxdump'))

rule genes_faa:
    input: expand(work_dir + "/prokka/{orgid}/{orgid}.renamed.faa", \
                  orgid = orgid_to_genome.keys(), **locals())
    output: faa_file
    benchmark: 'benchmarks/concatenate_faa.time'
    #shell: "rm {output}; for F in {input} ; do echo $F >> {output}; done"
    run:
        if len(input) == 0:
            raise Exception("There are no input files!!!")
        input_iter = iter(input)
        input_file = next(input_iter)
        shell("cat {input_file} > {output}")
        for input_file in input_iter:
            shell("cat {input_file} >> {output}")

rule rename_faa:
    input:
        faa=work_dir + "/prokka/{orgid}/{orgid}.faa",
        tax_table=tax_table
    output: work_dir + "/prokka/{orgid}/{orgid}.renamed.faa"
    run:
        # the tax table has already been parsed, so let's not re-do it here
        lineage = genome_taxonomy[orgid_to_genome[wildcards.orgid]]
        lineage = re.sub(' ','_',lineage)
        with open(output[0], 'wt') as out_handle:
            gene_count = 0
            for gene in SeqIO.parse(input.faa, 'fasta'):
                gene_count += 1
                gene_id, function = gene.description.split(None, 1)
                gene_no = int(gene.id.split("_", 1)[1])
                gene.id = '{org}_{N:07d}'.format(org=wildcards.orgid, N=gene_no)
                gene.description = '{tax} {function}' \
                    .format(tax=lineage, function=function)
                out_handle.write(gene.format('fasta'))
        logger.debug("renamed %d genes in %s" % (gene_count, wildcards.orgid))

def get_domain(wildcards):
    return genome_taxonomy[orgid_to_genome[wildcards.orgid]] \
            .split(";")[0].split("__")[1]

rule prokka:
    input: lambda w: \
        "{work_dir}/release{rel}/fastani/database/{genome}_genomic.fna" \
        .format(genome=orgid_to_genome[w.orgid], work_dir=work_dir, rel=rel)
    output: work_dir + "/prokka/{orgid}/{orgid}.faa"
    benchmark: "benchmarks/prokka/{orgid}/{orgid}.faa"
    params:
        kingdom=get_domain,
        outdir=lambda w: "{work_dir}/prokka/{orgid}" \
                    .format(work_dir=work_dir, orgid=w.orgid)
    threads: config.get("prokka_threads", 5)
    shell: """
        prokka --outdir={params.outdir} --prefix {wildcards.orgid} \
               --kingdom={params.kingdom} --force {input} --cpus {threads} \
            > {output}.log 2>&1
            """

rule gunzip:
    input: "{gzroot}_genomic.fna.gz"
    output: "{gzroot}_genomic.fna"
    shell:
        "gunzip -c {input} > {output}"

rule unpack:
    input: gtdb_tarball
    output: expand( \
        "{work_dir}/release{rel}/fastani/database/{genome}_genomic.fna.gz", \
        genome=genome_taxonomy.keys(), **locals())
    shell:
        "cd {work_dir} && tar -zxf {input} && \
            find release{rel} -type f | xargs touch"
 
