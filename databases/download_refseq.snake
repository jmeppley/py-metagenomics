##########################
#
# download_refseq.snake
#
# Downloads and formats the latest RefSeq database for last and/or diamond
#
##########################
import datetime
import re
import os
import subprocess
import urllib.request
from ftplib import FTP
from functools import partial

import pandas
from Bio import SeqIO
from edl import taxon
from edl import  util

from snakemake.remote.FTP import RemoteProvider as FTPRemoteProvider
FTPR = FTPRemoteProvider()

# FTP locations
FTP_SERVER = 'ftp.ncbi.nlm.nih.gov'
RELEASE_PATH = 'refseq/release'
FTP_REL = f'ftp://{FTP_SERVER}/{RELEASE_PATH}'
TAXDUMP_PATH = 'pub/taxonomy/taxdump_archive'
FTP_TAX = f'ftp://{FTP_SERVER}/{TAXDUMP_PATH}'

###
# release number
#
# this doesn't work as a checkpoint because
# format_dbs.snake needs to know all the file names ahead of time
# for the wildcard_constraints to work
def get_refseq_release():
    """
    reimpliment make line:
    REL?=$(shell curl $(FTP_REL)/RELEASE_NUMBER)
    """
    # read file contents
    with urllib.request.urlopen(f"{FTP_REL}/RELEASE_NUMBER") as response:
        release_data = response.read()

    # make into a string for regex
    if isinstance(release_data, bytes):
        release_data = release_data.decode()

    return release_data.strip()

# version number
release = config.get('release', get_refseq_release())

# where to put the downloaded files:
seqdb_root = config.get('seqdb_root','seqdbs')
download_root = config.get('download_root', seqdb_root)

# file locations
REFSEQ_DL_DIR = os.path.join(download_root, 'RefSeq')
SEQDB_DIR = os.path.join(seqdb_root, 'RefSeq', release)
NAME_ROOT = f"{SEQDB_DIR}/RefSeq-{release}.AllProteins"
GENES_FAA = NAME_ROOT + ".faa"
# taxonomy files
ACCS_TAXIDS = SEQDB_DIR + "/taxdump/acc.to.taxid"
NAMES_DMP = SEQDB_DIR + "/taxdump/names.dmp"
NODES_DMP = SEQDB_DIR + "/taxdump/nodes.dmp"

# other config
fmt_threads = config.setdefault('fmt_threads', 20)

###
# final targets

config['metadata_templates'] = {
    f'RefSeq-{release}.stats.txt': \
        'release-statistics/RefSeq-release{release}.{release_date:%m%d%Y}.stats.txt',
    f'RefSeq-{release}.txt': 'release-notes/RefSeq-release{release}.txt'}

# formatting for last and diamond happends here
#  we give format_dbs executables instead of paths to wait for the release
#  number
config['dbs'] = {'RefSeq': {'fasta': GENES_FAA, \
                          'is_prot': True, \
                          'links': {'.tax': ACCS_TAXIDS, \
                                    'names.dmp': NAMES_DMP, \
                                    'nodes.dmp': NODES_DMP, \
                                    '.ids': True}}}

# default is last8 and diamond change with config['db_fmts']
include: 'format_dbs.snake'

# all rule: define what files need to be created
#   get_formatted_db_outputs defined by format_dbs.snake
rule outputs:
    input:
        formatting_outputs=config.get('outputs', {}),
        catalog_metadata=f'{SEQDB_DIR}/metadata/.done.catalog',
        other_metadata=expand('{SEQDB_DIR}/metadata/{file_name}', \
               SEQDB_DIR=SEQDB_DIR, \
               file_name=config['metadata_templates'].keys()),
        fragment_counts=f'{SEQDB_DIR}/complete/.counts.checked'

###
# Checkpoints

# notes have the release date
checkpoint release_notes:
    input:
        FTPR.remote(f'{FTP_SERVER}/{RELEASE_PATH}/release-notes/' \
                    'RefSeq-release{release}.txt')
    output: f'{REFSEQ_DL_DIR}/release-notes/RefSeq-{{release}}.txt'
    shell: """ ls -lrth {input}
               cp {input} {output}
           """

# gene files
checkpoint get_pep_list:
    """ get the list of protein files to download o
        we could use FTPR.glob_wildcards, but that adds 7 seconds to every re-run instead of checking the local filesystem
    """
    output: f"{SEQDB_DIR}/complete/pep.gz.list"
    shell: """curl -s -l ftp://{FTP_SERVER}/{RELEASE_PATH}/complete/ | grep "protein\.gpff" > {output}"""


###
# checkpoint functions
#  - functions to get filenames and other strings that depend on checkpoints

# get release date
def get_release_date():
    """ we need this for the taxdump and the stats file """
    release_notes_file = checkpoints.release_notes.get(release=release).output[0]
    with open(release_notes_file) as rel_notes_lines:

        # find the first line that looks like a date
        for line in rel_notes_lines:
            m = re.search(f'([A-Z][a-z]+\s+\d+,\s+\d\d\d\d)', line)
            if m:
                # convert to datetime object (to be formatted later)
                return datetime.datetime.strptime(m.group(1),
                                                  "%B %d, %Y")

def get_taxdump_file():
    """ find the newst taxdump from before the release """
    ftp = FTP(FTP_SERVER)
    ftp.login()
    ftp.cwd(TAXDUMP_PATH)

    refseq_date = get_release_date()

    taxdump_list = []
    ftp.retrlines('LIST', taxdump_list.append)           # list directory contents

    best_date = None
    best_file = None
    for line in taxdump_list:
        try:
            file_name, file_date = \
                re.search(r'(taxdmp_(\d\d\d\d-\d+-\d+).zip)', line).groups()
            file_date = datetime.datetime.strptime(file_date, '%Y-%m-%d')
            if file_date < refseq_date:
                if best_date is None or file_date > best_date:
                    best_date = file_date
                    best_file = file_name
        except:
            pass

    return best_file

# get list of complete fragment names
def get_complete_files():
    gz_list = checkpoints.get_pep_list.get(release=release).output[0]
    with open(gz_list) as gz_files:
        return [g.strip() for g in iter(gz_files) \
                if re.search(r'^complete\..+\.protein.gpff.gz$', g)]

def get_complete_outputs(suffix, wildcards):
    return [f'{SEQDB_DIR}/complete/' + re.sub(r'gpff.gz$', suffix, f) for f in get_complete_files()]


# metadata

def get_md_remote(wildcards):
    file_name = wildcards.file_name
    input_template = config['metadata_templates'][file_name]
    if re.search('release_date', input_template):
        input_path = input_template.format(release=release,
                                           release_date=get_release_date())
    else:
        input_path = input_template.format(release=release)

    return FTPR.remote(f"{FTP_SERVER}/{RELEASE_PATH}/{input_path}")

###
# Rules

# get the NCBI taxdump
rule taxdump:
    input:
        lambda w:
            FTPR.remote(f'{FTP_SERVER}/{TAXDUMP_PATH}/{get_taxdump_file()}')
    output:
        names=NAMES_DMP,
        nodes=NODES_DMP
    shell:
        """
        ZIP=$(realpath {input})
        cd {SEQDB_DIR}/taxdump
        unzip $ZIP
        """

# other metadata files
rule metadata_file:
    input: get_md_remote
    output: f'{SEQDB_DIR}/metadata/{{file_name}}'
    shell: "cp {input} {output}"

rule catalog_metadata:
    """ a bunch of files we don't use explicitly, but might be useful """
    output:
        f'{SEQDB_DIR}/metadata/.done.catalog'
    shell: """
        cd {SEQDB_DIR}/metadata
        wget -c {FTP_REL}/release-catalog/[Rr]*
        cd -
        touch {output}
        """

rule dl_proteins:
    """
    Using shell instead of FTP because building the DAG crashes with 2200
    FTP files: ftputil.error.FTPOSError: [Errno 24] Too many open files
    input:
        lambda w:
            FTPR.remote(f"{FTP_SERVER}/{RELEASE_PATH}/complete/{w.filename}.gpff.gz",
                        immediate_close=True)
	"""
    output: f"{SEQDB_DIR}/complete/{{filename}}.gpff"
    params:
        url=lambda w:
           f"{FTP_REL}/complete/{w.filename}.gpff.gz"
    #shell: "gunzip -c {input} > {output}"
    shell: """ curl -s {params.url} \
                | gunzip -c \
                > {output} """


rule extract_proteins:
    input: f"{SEQDB_DIR}/complete/{{filename}}.gpff"
    output: f"{SEQDB_DIR}/complete/{{filename}}.faa"
    threads: 2
    shell: "cat {input} \
                | grep -v Assembly-Data \
                | get_sequences_from_gb.py -F fasta -r \
                | tantan -p \
                > {output}"

rule count_fragment_genes:
    input:
        gb=f"{SEQDB_DIR}/complete/{{filename}}.gpff",
        faa=f"{SEQDB_DIR}/complete/{{filename}}.faa"
    output: f"{SEQDB_DIR}/complete/{{filename}}.counts"
    threads: 2
    shell: """
        prinseq-lite.pl -aa -fasta {input.faa} -stats_info > {output}
        # prinseq doesn't do exit codes. Fail if output empty
        if [ ! -s {output} ]; then false; fi
        LOCI=$(grep -c "^LOCUS" {input.gb})
        echo -e "gb_file\tloci\t$LOCI" >> {output}

        READS=$(grep reads {output} | cut -f 3)
        if [ "$READS" != "$LOCI" ]; then
            echo "sequences counts are different from genbank and fasta files"
            echo {input}
            false
        fi
        """

rule genes_counts:
    input: partial(get_complete_outputs, "counts")
    output: f'{SEQDB_DIR}/complete/counts.txt'
    shell:
        """
        echo -e "READS\tLOCI" > {output}
        cat {SEQDB_DIR}/complete/*counts \
            | perl -lane 'if ($F[1] eq "reads")
                            {{$read = $F[2]; $reads += $read}}
                          elsif ($F[1] eq "loci")
                            {{$locus = $F[2]; $loci += $locus}};
                          print "$reads\t$loci";' \
            | tail -n 1 \
            >> {output}
        """

rule check_gene_counts:
    input:
        final_counts=GENES_FAA + ".prinseq.stats",
        fragment_totals=f'{SEQDB_DIR}/complete/counts.txt'
    output:
        f'{SEQDB_DIR}/complete/.counts.checked'
    run:
        # read one row table with headers into dict
        fragment_sums = pandas.read_csv(input.fragment_totals, sep='\t') \
                              .T[0].to_dict()

        if fragment_sums['READS'] != fragment_sums['LOCI']:
            raise Exception("Total faa genes don't match total gb genes! \n" \
                            f" {fragment_sums['READS']} is not " \
                            f"{fragment_sums['LOCI']}")

        # read second and third columsn as keys and values into dict
        prinseq_counts = pandas.read_csv(input.final_counts, sep='\t', \
                                         header=None, usecols=[1,2], \
                                         names=['key','value'], \
                                         index_col=0)['value'].to_dict()

        if fragment_sums['READS'] != prinseq_counts['reads']:
            raise Exception("Total faa genes don't final faa count! \n" \
                            f" {fragment_sums['READS']} is not " \
                            f"{prinseq_counts['reads']}")

        with open(output[0], 'wt') as out:
            out.write("checks out")


rule genes_faa:
    input: partial(get_complete_outputs, "faa")
    output: GENES_FAA
    run:
        with open(output[0], 'wt') as genes_out:
            for faa_file in input:
                with open(faa_file) as genes_in:
                    for line in genes_in:
                        genes_out.write(line)

""" MAKE rule:
$(ACCPREFP): $(RSDIR)/complete/.download.complete.aa
	# For some multispecies entries, you'll get multiple lines in the tax map
	gunzip -c $(RSDIR)/complete/complete.*.protein.gpff.gz \
       | perl -ne 'if (m/^ACCESSION\s+(\S+)\b/) { $$acc=$$1; }
                  elsif (m/db_xref="taxon:(\d+)"/) { print "$$acc\t$$1\n"; }' \
       > $@
"""
rule acc_taxid_map:
    input: partial(get_complete_outputs, "taxid")
    output: ACCS_TAXIDS
    run:
        with open(output[0], 'wt') as taxids_out:
            for taxid_file in input:
                with open(taxid_file) as taxids_in:
                    for line in taxids_in:
                        taxids_out.write(line)

ACC_REXP = re.compile(r'^ACCESSION\s+(\S+)\b')
TAXID_REXP = re.compile(r'db_xref="taxon:(\d+)')

rule extract_taxids:
    input: f"{SEQDB_DIR}/complete/{{filename}}.gpff"
    output: f"{SEQDB_DIR}/complete/{{filename}}.taxid"
    run:
        with open(output[0], 'wt') as output_handle:
            with open(input[0]) as input_handle:
                for line in input_handle:
                    m = ACC_REXP.search(line)
                    if m:
                        acc = m.group(1)
                        continue

                    m = TAXID_REXP.search(line)
                    if m:
                        taxid = m.group(1)
                        output_handle.write(f"{acc}\t{taxid}\n")
                        continue
